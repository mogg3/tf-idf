{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "subjective-cruise",
   "metadata": {},
   "source": [
    "                                                                      Marcus Eriksson, Mattias Barth, Sara Kaminska\n",
    "\n",
    "# TF-IDF - vad är det och hur fungerar det?\n",
    "TF-IDF står för Term Frequency - Inverse Document Frequency och är en teknik som används för att statistiskt avgöra hur relevant ett ord är för ett dokument i en dokumentsamling. Tekniken används vid automatiserad textanalys i bland annat sökmotorer.\n",
    "\n",
    "Term Frequency är den delen av tekniken som beräknar hur vanligt förekommande ett ord är i ett dokument. I beräkningen tas det hänsyn till dokumentets omfattning, genom att dividera antalet gånger ordet förekommer med det totala antalet ord. \n",
    "\n",
    "    TF = antalet gånger ordet förekommer i dokumentet / totala antalet ord i dokumentet\n",
    "\n",
    "Inverse Document Frequency har som uppgift att värdera hur relevant ett ord är för ett dokument. En text innehåller många så kallade stop-ord, exempelvis I, we, it, did, here, vilket leder till att de får ett högt TF-värde. Att ta hänsyn till dessa stop-ord i analysen ger emellertid ett vilseledande resultat. Det är här IDF kommer in i bilden. Tekniken kontrollerar hur många dokument i en samling innehåller det givna ordet. Vidare divideras det totala antalet dokument i samlingen med antalet dokument som ordet förekommer i. Eftersom värdena kan bli väldigt höga för större dokumentsamlingar används logaritmen av 10, för att minska de. Beräkningen resulterar i att ord som förekommer i färre dokument får en högre siffra, och därmed ett högre värde. \n",
    "\n",
    "\tIDF = antalet dokument i samlingen / antalet dokument som innehåller det givna ordet\n",
    "\n",
    "Det slutliga värdet för TF-IDF räknas ut genom att multiplicera TF (antalet gånger ett ord förekommer i en text)  med IDF (ordets vikt). Ett ord som förekommer ofta i många dokument kommer att få ett lägre TF-IDF, medan ett ord som förekommer ofta i något enstaka dokument får ett högre, vilket tolkas som att ordet har en hög relevans för dokumentet. \n",
    "\n",
    "\tTF-IDF = TF・IDF\n",
    "\n",
    "# Hur fungerar TF-IDF i praktiken?\n",
    "\n",
    "Det första som görs är att analysera det \"dataset\" man har för att veta hur datan skall kunna jämföras, analyseras och tas fram. Den data som skall analyseras är inte alltid trivial att ta fram och därför bör det åtminstone göras en mindre analys av datan (som kan göras manuellt). När den första analysen är klar och man har hittat ett sätt att få fram sin data behöver man processa den aktuella datan. Datan behöver tvättas och omvandlas då maskininlärningsalgoritmer inte direkt kan arbeta med råtext. Texten behöver konverteras till vektorer som algoritmen klarar av att använda. De metoder som används för att tvätta data varier från fall till fall. Några exempel på metoder för datatvätt av en text är:\n",
    "\n",
    "- gör all text till \"lover case\"\n",
    "- ersätta skiljetecken (\" . , # _) i texten till blanksteg\n",
    "- ersätta ord med bara ett tecken till blanksteg\n",
    "- ta bort \"stop words\" från texten. \"Stop words\" är de mest vanliga orden i språket och ger inget användbart värde. Vidare öka effektiviteten i beräkningar och i utrymme om dessa “stop words” tas bort.\n",
    "- \"stemming\", där längre ord omvandlas till sin stam\n",
    "- \"lemmatisation, där ord reduceras till \"root synonym\"\n",
    "\n",
    "När datatvätten är klar kan beräkningar och analyser göras. \n",
    "\n",
    "\n",
    "# För- och nackdelar med TF-IDF\n",
    "Nedan analys bygger på våra erfarenheter av TF-IDF. De fördelar som vi upplever med tekniken är att det är ett bra och grundläggande verktyg med en enkel algoritm som ger en god generell bild av vikten av sökta ord/dokument mot ett \"corpus\". TF-IDF verkar vara en bra start för att börja förstå grunderna inom Machine Learning. \n",
    "\n",
    "\n",
    "Några nackdelar som vi upptäckt under tiden som vi arbetat med TF-IDF är att tekniken kräver ett stort corpus för att ge bra output. Ett stort corpus medför att det tar tid att göra beräkningarna då det är många dokument att jämföra med.Ytterligare en nackdel är att man måste veta med vilka metoder man skall tvätta sin data, dels för att för att kunna få fram relevanta resultat, dels att det riskerar att bli tungt att köra annars. Vi upplevde även att det var svårt att tolka resultatet och veta exakt vad de siffervärden vi får fram innebär. \n",
    "\n",
    "\n",
    "# Bokrekommendation \n",
    "Om vi hade haft mer tidsutrymme hade vi angripit uppgiften om bokrekommendation enligt nedan. Vi hade börjat med att skapa en dokumentsamling med böcker som vi kan göra våra rekomendationer utifrån. Vidare hade vi gjort en TF-IDF analys på samtliga och lagrat resultaten. När en användare sedan ber om en bokrekommendation vektoriseras samtliga dokument som finns sparade och de med högst cosine similarity, det vill säga högst likhet, rekomenderas till användaren. Ytterligare ett villkor som förbättrar bokrekommendationerna och effektiviserar sökningen är att även lagra information om genre för varje bok. Vår algoritm skulle isåfall först begränsa sökningen till böckerna i samma genre, och endast beräkna cosine similarity för dessa. \n",
    "\n",
    "\n",
    "# Källförteckning\n",
    "Toward Data Science, TF-IDF from scratch in python on real world dataset, hämtad 2021-03-11 från\n",
    "https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-pythonon-real-world-dataset-796d339a4089\n",
    "\n",
    "Towards Data Science, TF-IDF Python Example, hämtad 2021-03-14 från https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecological-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-vienna",
   "metadata": {},
   "source": [
    "Koden nedan ber användaren om en boktitel att söka på. Sedan görs sökning mot websidan gutenberg.org och användaren presenteras de 25 mest populära böckerna som matchar sökningen. Boken tvättas (detta förklaras djupare längre ner) och sparas ner i en fil med hjälp av biblioteket pickle. Processen upprepas en gång till för att få en titel att jämföra med.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "designed-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_books():\n",
    "    for i in range(2):\n",
    "        search = input(\"Search: \")\n",
    "        book_lines = scrape(search, i)\n",
    "        cleaned = clean_book(book_lines)\n",
    "        with open(f'documents/book{i+1}.py', 'wb') as file:\n",
    "            pickle.dump(cleaned, file)\n",
    "\n",
    "\n",
    "def scrape(title, j):\n",
    "    driver = webdriver.Chrome('./chromedriver')\n",
    "    driver.get('https://www.gutenberg.org/')\n",
    "    search_field = driver.find_element_by_id('menu-book-search')\n",
    "    search_field.send_keys(title.lower())\n",
    "    search_field.send_keys(Keys.RETURN)\n",
    "    results = WebDriverWait(driver, 10).until(EC.visibility_of_all_elements_located((By.CLASS_NAME, 'booklink')))\n",
    "    titles = [result.find_element_by_class_name(\"title\").text for result in results]\n",
    "\n",
    "    for i, book in enumerate(titles):\n",
    "        print(f\"{i + 1}. \" + book)\n",
    "\n",
    "    choice = int(input(\"What book were you thinking of?(1-25): \"))\n",
    "\n",
    "    chosen_book = results[choice+1]\n",
    "    chosen_book.click()\n",
    "    text = WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.LINK_TEXT, 'Plain Text UTF-8')))\n",
    "    text.click()\n",
    "    book_text = driver.page_source\n",
    "    with open(f\"documents/book{j+1}.py\", \"w\") as text_file:\n",
    "        text_file.write(book_text)\n",
    "    with open(f\"documents/book{j+1}.py\", \"r\") as text_file:\n",
    "        book_lines = text_file.readlines()\n",
    "    return book_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-capture",
   "metadata": {},
   "source": [
    "Här tvättas boktexterna. Följande processer går texten igenom: \n",
    "    - Plocka ut det faktiska boktexten. Vi tar bort all text som inte hör boken till, exempelvis policy.\n",
    "    - Tar bort radbrytningar. Eftersom boken ligger i en textfil (ex. bok.txt) ligger radbrytningarna('\\n') som en del       av texten när raderna läses in som enskilda element i en lista. Dessa vill vi bli av med. \n",
    "    - Listan med rader görs om till en sträng med all text. \n",
    "    - Samtliga tecken som inte är bokstäver tas bort från texten. Apostrofer tas även bort. Don't ---> Dont\n",
    "    - Texten delas upp ord för ord och sparas i en lista.\n",
    "    - Eventuella element i listan som bara består av mellanslag tas bort. Mellanslag i början och/eller slutet av           orden tas även bort. \n",
    "    - Enbokstavsord tas bort från listan, då detta inte bidrar med någon semantisk vikt till texten\n",
    "    - Stopord tas bort från texten då dessa heller inte ger någon semantisk vikt till texten. Ord såsom The, Them, I,       Me, My etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-speaking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_text(lines):\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"*** START OF\" in line or \"***START OF\" in line:\n",
    "            start = i + 1\n",
    "        if \"*** END OF\" in line or \"***END OF\" in line:\n",
    "            end = len(lines) - i\n",
    "    book = lines[start:-end:]\n",
    "    return book\n",
    "\n",
    "\n",
    "def remove_line_br(book):\n",
    "    book = [line for line in book if line != '\\n']\n",
    "    for i, line in enumerate(book):\n",
    "        if \"\\n\" in line:\n",
    "            book[i] = line.rstrip(\"\\n\")\n",
    "    return book\n",
    "\n",
    "\n",
    "def to_string(book):\n",
    "    book_str = \"\"\n",
    "    for line in book:\n",
    "        book_str += line + \" \"\n",
    "    return book_str\n",
    "\n",
    "\n",
    "def no_symbols(book_string):\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    unsymboled_string = \"\"\n",
    "    for character in book_string:\n",
    "        if character == \"'\":\n",
    "            unsymboled_string += \"\"\n",
    "        elif character not in alphabet:\n",
    "            unsymboled_string += \" \"\n",
    "        else:\n",
    "            unsymboled_string += character.lower()\n",
    "    return unsymboled_string\n",
    "\n",
    "\n",
    "def to_word_list(book_string):\n",
    "    return book_string.split(\" \")\n",
    "\n",
    "\n",
    "def no_blanks(word_list):\n",
    "    no_blanks_word_list = []\n",
    "    for word in word_list:\n",
    "        if word != \" \":\n",
    "            no_blanks_word_list.append(word)\n",
    "    for i, line in enumerate(no_blanks_word_list):\n",
    "        if \" \" in line:\n",
    "            no_blanks_word_list[i] = line.rstrip(\" \")\n",
    "    return no_blanks_word_list\n",
    "\n",
    "\n",
    "def remove_one_letter_words(words_list):\n",
    "    no_one_letter_words_list = [word for word in words_list if len(word) > 1]\n",
    "    return no_one_letter_words_list\n",
    "\n",
    "\n",
    "def remove_stopwords(word_list):\n",
    "    stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'youre', 'youve', 'youll', 'youd', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'shes', 'her', 'hers', 'herself', 'it', 'its', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'thatll', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'dont', 'should', 'shouldve', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'arent', 'couldn', 'couldnt', 'didn', 'didnt', 'doesn', 'doesnt', 'hadn', 'hadnt', 'hasn', 'hasnt', 'haven', 'havent', 'isn', 'isnt', 'ma', 'mightn', 'mightnt', 'mustn', 'mustnt', 'needn', 'neednt', 'shan', 'shant', 'shouldn', 'shouldnt', 'wasn', 'wasnt', 'weren', 'werent', 'won', 'wont', 'wouldn', 'wouldnt']\n",
    "    no_stopwords_word_list = [word for word in word_list if word not in stopwords]\n",
    "    return no_stopwords_word_list\n",
    "\n",
    "def clean_book(book_lines):\n",
    "    data = remove_unwanted_text(book_lines)\n",
    "    data = remove_line_br(data)\n",
    "    data = to_string(data)\n",
    "    data = no_symbols(data)\n",
    "    data = to_word_list(data)\n",
    "    data = no_blanks(data)\n",
    "    data = remove_one_letter_words(data)\n",
    "    cleaned_data = remove_stopwords(data)\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-cache",
   "metadata": {},
   "source": [
    "Funktionerna laddar in filen och sparar den i en lista för vidare hantering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents():\n",
    "    with open(\"documents/book1.py\", \"rb\") as book1:\n",
    "        book1 = pickle.load(book1)\n",
    "    with open(\"documents/book2.py\", \"rb\") as book2:\n",
    "        book2 = pickle.load(book2)\n",
    "    return book1, book2\n",
    "\n",
    "def get_corpus():\n",
    "    corpus_list = []\n",
    "    for file in os.listdir('./corpus'):\n",
    "        with open('./corpus/' + file, 'rb') as book_file:\n",
    "            book_file = pickle.load(book_file)\n",
    "            corpus_list.append(book_file)\n",
    "    return corpus_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-webmaster",
   "metadata": {},
   "source": [
    "Funktionerna ansvarar för att utföra samtliga beräkningar som krävs för att få fram ett TF-IDF värde. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-october",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf(book):\n",
    "    tf = {}\n",
    "    for word in book:\n",
    "        if word in tf:\n",
    "            tf[word] += 1\n",
    "        else:\n",
    "            tf[word] = 1\n",
    "    for key in tf:\n",
    "        tf[key] = tf[key]/len(book)\n",
    "    return tf\n",
    "\n",
    "\n",
    "def calculate_df(book, corpus):\n",
    "    df = {}\n",
    "    for i, word in enumerate(book):\n",
    "        print(f\"{i+1}/{len(book)}\")\n",
    "        df[word] = 0\n",
    "        for corpus_document in corpus:\n",
    "            if word in corpus_document:\n",
    "                df[word] += 1\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_idf(df):\n",
    "    n = len(os.listdir('./corpus'))\n",
    "    for key in df:\n",
    "        df[key] = math.log(n/(df[key]+1), 10)\n",
    "    idf = df\n",
    "    return idf\n",
    "\n",
    "\n",
    "def calculate_tf_idf(tf, idf):\n",
    "    tf_idf = {}\n",
    "    for key in tf:\n",
    "        tf_idf[key] = tf[key] * idf[key]\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-negative",
   "metadata": {},
   "source": [
    "Slutligen analyseras TF-IDF värdet genom att vektorisera böckerna och sedan beräkna vinkeln emellan dem. Detta görs med hjälp av följande förhållande: a ∙ b = |a||b| cos(v) där a och b är vektorer och v är vinkeln emellan dem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_vocab(book1, book2, corpus):\n",
    "    total_vocab = []\n",
    "    for corpus_document in corpus:\n",
    "        total_vocab += corpus_document\n",
    "    total_vocab += book1 + book2\n",
    "    return list(set(total_vocab))\n",
    "\n",
    "\n",
    "def vectorize_book(current_book, total_vocab):\n",
    "    book_vector = []\n",
    "    for word in total_vocab:\n",
    "        if word in current_book:\n",
    "            current_tfidf = current_book[word]\n",
    "            value = current_tfidf\n",
    "        else:\n",
    "            value = 0\n",
    "\n",
    "        book_vector.append(value)\n",
    "\n",
    "    return book_vector\n",
    "\n",
    "\n",
    "def calculate_cosine_similarity(a, b):\n",
    "    print(\"Len a = \", len(a))\n",
    "    print(\"Len b = \", len(b))\n",
    "\n",
    "    dot_product = sum([a[i]*b[i] for i in range(len(a))])\n",
    "    absolute = math.sqrt(sum([i**2 for i in a]) * sum([i**2 for i in b]))\n",
    "\n",
    "    return math.degrees(math.acos(dot_product/absolute))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-prisoner",
   "metadata": {},
   "source": [
    "Nedan körs alla nödvändiga funktioner och returnerar en uppskattning av böckernas likhetsgrad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_books(book1, book2, corpus):\n",
    "    tf1 = calculate_tf(book1)\n",
    "    df1 = calculate_df(book1, corpus)\n",
    "    idf1 = calculate_idf(df1)\n",
    "    tf_idf1 = calculate_tf_idf(tf1, idf1)\n",
    "\n",
    "    tf2 = calculate_tf(book2)\n",
    "    df2 = calculate_df(book2, corpus)\n",
    "    idf2 = calculate_idf(df2)\n",
    "    tf_idf2 = calculate_tf_idf(tf2, idf2)\n",
    "\n",
    "    total_vocab = get_total_vocab(book1, book2, corpus)\n",
    "\n",
    "    book1_vector = vectorize_book(tf_idf1, total_vocab)\n",
    "    book2_vector = vectorize_book(tf_idf2, total_vocab)\n",
    "    cosine_similarity = get_cosine_similarity(book1_vector, book2_vector)\n",
    "\n",
    "    appreciation = None\n",
    "\n",
    "    if cosine_similarity == 0:\n",
    "        appreciation = \"Böckerna är likadana\"\n",
    "    elif 0 < cosine_similarity <= 10:\n",
    "        appreciation = \"Böckerna har väldigt många likheter\"\n",
    "    elif 10 < cosine_similarity <= 45:\n",
    "        appreciation = \"Böckerna har många likheter\"\n",
    "    elif 45 < cosine_similarity <= 60:\n",
    "        appreciation = \"Böckerna har vissa likheter\"\n",
    "    elif 60 < cosine_similarity <= 80:\n",
    "        appreciation = \"Böckerna har få likheter\"\n",
    "    elif 80 < cosine_similarity:\n",
    "        appreciation = \"Böckerna har väldigt få likheter\"\n",
    "    return appreciation, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "smooth-renaissance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search: TWILIGHT\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'webdriver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a50fed0cd13a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_books\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbook1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbook2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mappreciation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosine_similarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompare_books\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbook1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbook2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Appreciation: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappreciation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-eacb465b609b>\u001b[0m in \u001b[0;36msave_books\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Search: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mbook_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mcleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_book\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbook_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'documents/book{i+1}.py'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-eacb465b609b>\u001b[0m in \u001b[0;36mscrape\u001b[0;34m(title, j)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./chromedriver'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://www.gutenberg.org/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0msearch_field\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'menu-book-search'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'webdriver' is not defined"
     ]
    }
   ],
   "source": [
    "save_books()\n",
    "book1, book2 = get_documents()\n",
    "corpus = get_corpus()\n",
    "appreciation, cosine_similarity = compare_books(book1, book2, corpus)\n",
    "print(\"Appreciation: \", appreciation)\n",
    "print(\"Cosine similarity: \", cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-motivation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
